{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd3363c-0e09-47f7-8953-9f2f887f624d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformer Based Batch Inference\n",
    "\n",
    "This notebook aims to show how you can run batch inference using Spark's distributed capabilites, with a multi-machine multi-gpu setup.\n",
    "\n",
    "Run on Databricks ML 14.0, with 4 A100 GPUs (1 Driver + 3 Worker) using _N24ads_A100_V4_ machines on Azure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632a9a3f-f2a3-4415-a9aa-b8780eafed00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Install & Upgrade Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96943394-f803-46e9-8cee-9336fe79a47d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers\n",
    "!pip install -q --upgrade accelerate\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3d7111-d60c-44ac-940e-e9127a4a3856",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GPU Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7fa476-5bf5-446d-a97d-502ddc2b26bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9033c85a-47bd-4cdf-a789-9f46ab5740a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Parameters\n",
    "\n",
    "The model name & tokenizer name below should be adjustable to most of the other models existing in the hugging face world.\n",
    "\n",
    "It would also make sense to change the prompt if there is a change to the model. This one is specifically designed for the LLAMA V2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74fd51cb-d946-4d34-8635-f5470d1cfd35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model Params\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "MAX_NEW_TOKENS = 300\n",
    "MIN_LENGTH = 0\n",
    "REPETITION_PENALTY = 1.2\n",
    "TEMPERATURE = 0.1\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "DO_SAMPLE = True\n",
    "USE_CACHE = True\n",
    "\n",
    "# Tokenizer Params\n",
    "TOKENIZER_NAME = MODEL_NAME\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "# Run Params (How many articles to use)\n",
    "MAX_EXAMPLES = 10000\n",
    "\n",
    "# Storage Params\n",
    "STORAGE_PATH = \"/dbfs/llm-examples\"\n",
    "\n",
    "# Instruction\n",
    "INSTRUCTION = \"\"\"Please provide a concise summary for the following article: {text}\"\"\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT_TEMPLATE = f\"\"\"<s>[INST]<<SYS>>\n",
    "You are a direct and honest assistant. Please provide concise and factual answers and just the answers.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "{INSTRUCTION}\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c61ab44-949e-4ee6-9dc4-2869a49c30d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Storage Operations\n",
    "\n",
    "Makes sure that the directory is cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6921db-f3bf-4c43-9571-1728a231122c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Remove existing files\n",
    "shutil.rmtree(\"/dbfs/llm-examples\")\n",
    "\n",
    "# Build the new directory\n",
    "os.makedirs(\"/dbfs/llm-examples\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4be6d00-bd7f-463f-b4dd-ed8672b29ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Huggingface Login\n",
    "\n",
    "This step can be skipped if your model doesn't require a login. LLAMA V2 does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a57d541-4da7-428a-ac83-b7f7d511434a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Login to hugging face\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login the huggingface\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "973d0f78-5c75-4aa4-b892-9a4b8d36570a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Retrieve Articles Data\n",
    "\n",
    "CNN & Daily Mail articles data set from Hugging Face Datasets is retrieved and combined to make a pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4ee291-7387-45f8-b5e3-c070870ff7da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets.utils import logging as dataset_logging, disable_progress_bar\n",
    "from pyspark.sql import functions as SF\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Disable verbose loggers\n",
    "dataset_logging.set_verbosity_error()\n",
    "disable_progress_bar()\n",
    "\n",
    "# Download dataset\n",
    "dataset = load_dataset(\n",
    "    path=\"cnn_dailymail\", name=\"3.0.0\", cache_dir=f\"{STORAGE_PATH}/hf\"\n",
    ")\n",
    "\n",
    "# Create spark dataframes\n",
    "train_df = spark.createDataFrame(data=dataset[\"train\"].to_pandas())\n",
    "val_df = spark.createDataFrame(data=dataset[\"validation\"].to_pandas())\n",
    "test_df = spark.createDataFrame(data=dataset[\"test\"].to_pandas())\n",
    "\n",
    "# Union for all data\n",
    "articles_df = train_df.union(val_df).union(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ffe435-7e6a-4127-a5eb-d7b6c29a50e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sample Data\n",
    "\n",
    "Deterministic sampling through ID hexing for consistent results & benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b64b75b-f05d-4785-9654-fe5b9cac703e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "import hashlib\n",
    "\n",
    "# Build function for creating a random column\n",
    "@SF.udf(\"string\")\n",
    "def generate_hex_from_string(input_string: str) -> str:\n",
    "    sha256 = hashlib.sha256()\n",
    "    sha256.update(input_string.encode(\"utf-8\"))\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "\n",
    "# Generate random string\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"random_string\", generate_hex_from_string(SF.col(\"id\"))\n",
    ")\n",
    "\n",
    "# Order by randomness and limit dataframe size\n",
    "articles_df = articles_df.orderBy(SF.col(\"random_string\")).limit(MAX_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6d5212-466b-4d68-b2be-9bc29e1dfedd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate Prompts\n",
    "\n",
    "Prompts are applied with the article text to generate prepared instructions for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d8ca41-0af4-41e8-b954-44218c2da0e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Build function for generating instructions\n",
    "@SF.udf(\"string\")\n",
    "def generate_instructions(article):\n",
    "    return PROMPT_TEMPLATE.format(text=article)\n",
    "\n",
    "\n",
    "# Generate instructions\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"instruction\", generate_instructions(SF.col(\"article\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62ed7e9-7f9f-4659-a659-6d51a5c65e61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Execute Data Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07bd080-a45e-45dd-85f1-87145e4da39e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trigger with action\n",
    "articles_df = spark.createDataFrame(articles_df.toPandas()).repartition(10)\n",
    "\n",
    "# Cache for performance\n",
    "articles_df.cache()\n",
    "print(f\"Number of Examples: {articles_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057d87e4-9e0a-4dab-8672-11f51fd5fb69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Download Model & Tokenizer\n",
    "\n",
    "Downloading the model and the tokizer helps when it comes to loading the model faster during the multi machine inference step.\n",
    "\n",
    "If the model and the tokenizer are in the same repository, only one download will occur. In some cases, for example for Falcon-7B, they can be different. In that case, the code downloads both to the location specified in params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e326895c-391c-4c1b-8fad-bf3fa9676cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from huggingface_hub.utils import (\n",
    "    disable_progress_bars as hfhub_disable_progress_bar,\n",
    "    logging as hf_logging,   \n",
    ")\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# Turn Off Info Logging for Transfomers\n",
    "hf_logging.set_verbosity_error()\n",
    "hfhub_disable_progress_bar()\n",
    "\n",
    "# Download the model \n",
    "local_model_path = f\"{STORAGE_PATH}/model/\"\n",
    "os.makedirs(local_model_path, exist_ok=True)\n",
    "model_download = snapshot_download(\n",
    "    repo_id=MODEL_NAME,\n",
    "    local_dir=local_model_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    ignore_patterns=\"*.safetensors*\", # This argument is specific to LLAMA. Other models might not need it.\n",
    "    max_workers=48\n",
    ")\n",
    "\n",
    "# Download the tokenizer\n",
    "if MODEL_NAME == TOKENIZER_NAME:\n",
    "    local_tokenizer_path = local_model_path\n",
    "else:\n",
    "    local_tokenizer_path = f\"{STORAGE_PATH}/tokenizer/\"\n",
    "    os.makedirs(local_tokenizer_path, exist_ok=True)\n",
    "    tokenizer_download = snapshot_download(\n",
    "        repo_id=TOKENIZER_NAME,\n",
    "        local_dir=local_tokenizer_path,\n",
    "        local_dir_use_symlinks=False,\n",
    "        max_workers=48\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea24216-9024-4ad1-b74f-629d72a38a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Model & Tokenizer\n",
    "\n",
    "Model and Tokenizer are loaded for the downloaded directory for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3c804b-c242-40e6-b2e9-3653f6b46152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Params\n",
    "random_seed = 42\n",
    "\n",
    "# Random seed set\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    return_dict=True,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1b16cb-d267-4b35-9f13-3fed6a2e4f07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run For One\n",
    "\n",
    "Batch Generate functions takes a list of prompts, and returns a list of ouputs (generated_text). Generation parameters such a temperature and top_p are set within the function.\n",
    "\n",
    "Even though this example shows how to do a few examples, this function will be used during distributed inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d49fd56-5c18-4c55-95eb-a5b7768cee69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "\n",
    "# Get sample data\n",
    "sample_instructions = [x[0] for x in articles_df.select(\"instruction\").limit(2).collect()]\n",
    "\n",
    "# Define Inference Flow\n",
    "@torch.inference_mode()\n",
    "def batch_generate(batch_prompts, tokenizer=tokenizer, model=model):\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        batch_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "        max_length=MAX_TOKENS\n",
    "    )\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "            temperature=TEMPERATURE,\n",
    "            min_length=MIN_LENGTH,\n",
    "            use_cache=USE_CACHE,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Check out one example\n",
    "print(batch_generate(sample_instructions[:1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646f6112-3d8c-4f12-916a-014627a58f82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Batch Test\n",
    "\n",
    "Optimal batch size changes depending on the GPU used. The GPU used in during this test has 80 GB of GPU memory, so going higher makes sense, however smaller machine like the A10s usually do better with smaller batch sizes.\n",
    "\n",
    "The code compares multiple batch sizes, and interation stops when out of memory error is raised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67371f1a-718b-4c72-9bbd-bed5202627ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "\n",
    "# Get sample data\n",
    "sample_instructions = [x[0] for x in articles_df.select(\"instruction\").limit(50).collect()]\n",
    "\n",
    "def batch_size_optimiser():\n",
    "    batch_sizes = [1, 2, 3, 4, 5, 7, 10, 12, 15, 17, 20, 25, 30]\n",
    "    success = True\n",
    "    for size in batch_sizes:\n",
    "        start = time.perf_counter()\n",
    "        try:\n",
    "            batch_generate(batch_prompts=sample_instructions[:size])\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            success = False\n",
    "            break\n",
    "        finally:\n",
    "            elapsed = round(time.perf_counter() - start, 2)\n",
    "            unit_time = round(elapsed/size, 2)\n",
    "            yield {\"batch_size\": size, \"elapsed_time\": elapsed, \"unit_time\": unit_time, \"success\": success}\n",
    "\n",
    "for result in batch_size_optimiser():\n",
    "    print(\"- - \" * 10)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc57e4e-32bc-4a81-9a23-5bf57f9bba14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Select Batch Size\n",
    "\n",
    "Doesn't necessarily has to be the largest batch size that succeeded without running into an OOM error. It is probably better to choose the 2nd or 3rd largest successful batch size so that OOM errors can be reduced during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731ca081-ddb5-4d1d-8f12-0539c3c79871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the bactch size with minimum unit time\n",
    "OPTIMAL_BATCH_SIZE = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f498bba5-156e-4bd3-b01c-ce3e17ac54c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Distributed Inference Logic\n",
    "\n",
    "All of the generation logic is carried into a Pandas UDF. This helps with the set up on the workers. An iterator to interator architecture is followed to handle batching processes. \n",
    "\n",
    "In the case that the model runs into an OOM Error, the function handles the exception by returnin a OOM string as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be596558-8558-437a-a62d-e75507dbc7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "\n",
    "# Build Inference Function\n",
    "@SF.pandas_udf(\"string\", SF.PandasUDFType.SCALAR_ITER)\n",
    "def run_distributed_inference(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "\n",
    "    # External Imports\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    # Params\n",
    "    random_seed = 42\n",
    "\n",
    "    # Random seed set\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        return_dict=True,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    for prompts in iterator:\n",
    "        prompts = prompts.to_list()\n",
    "        try:\n",
    "            output = batch_generate(\n",
    "                batch_prompts=prompts, \n",
    "                tokenizer=tokenizer, \n",
    "                model=model\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # If out of memory, return a series of OOM strings that has the lenght of the input\n",
    "            output = [\"OOM\"] * len(prompts)\n",
    "\n",
    "        yield pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc8047e-87ab-41f4-851d-9df089d9d9a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inference Configurations\n",
    "\n",
    "Automatically undertands how many workers are available in the cluster, and adjusts partitions accordingly. This means that the setup portion of the Pandas UDF which loads the model and tokenizer gets run only once during inference, and the data processing is handled with the iterator.\n",
    "\n",
    "Max Records Per Batch configuration controls how big the batch sizes are going to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80654ef-411d-490a-b683-aa605cdb205a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Auto get number of workers\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Subtract 1 to exclude the driver\n",
    "num_workers = len(sc._jsc.sc().statusTracker().getExecutorInfos()) - 1  \n",
    "\n",
    "# Set the batch size for the Pandas UDF\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", OPTIMAL_BATCH_SIZE * 2)\n",
    "\n",
    "# Repartition\n",
    "articles_df = articles_df.repartition(num_workers)\n",
    "\n",
    "# Cache DF\n",
    "articles_df.cache()\n",
    "articles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a037004-60c7-4e64-aab0-9165ff0c5c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run Distributed Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9c354e-24dd-48a8-be54-c96c453cf828",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Apply Inference UDF\n",
    "articles_df = (\n",
    "    articles_df\n",
    "    .withColumn(\"llm_summary\", run_distributed_inference(SF.col(\"instruction\")))\n",
    "\n",
    ")\n",
    "\n",
    "# Materilize and Execute\n",
    "inference_start_time = time.perf_counter()\n",
    "articles_pdf = articles_df.toPandas()\n",
    "inference_elapsed_time = round(time.perf_counter() - inference_start_time, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5622467-eca1-4ed7-83a0-de70a45fddf3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f147d8-c313-4409-ac9f-5f3c443401e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Go back to Spark\n",
    "articles_df = spark.createDataFrame(articles_pdf)\n",
    "\n",
    "# Cache DF\n",
    "articles_df.cache()\n",
    "articles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8c6d6f-dd6a-40be-8a84-6eced296c343",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb7e010-00d5-49f4-ae27-69db859527d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# UDF Build\n",
    "clean_llm_summary = SF.udf(lambda x: x.split(\"[/INST]\")[-1].strip(), \"string\")\n",
    "\n",
    "# Apply UDF\n",
    "articles_df = (\n",
    "    articles_df.withColumn(\n",
    "        \"cleaned_llm_summary\", clean_llm_summary(SF.col(\"llm_summary\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cb8325-721c-44ae-bca1-ff5db0704925",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calculate Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634f432c-e359-4729-8821-3c0141971cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@SF.udf(\"int\")\n",
    "def calculate_n_tokens(target_text):\n",
    "    return len(\n",
    "        tokenizer.encode_plus(\n",
    "            target_text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=True,\n",
    "            max_length=MAX_TOKENS,\n",
    "        )[\"input_ids\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Calculate article tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"article_token_count\", calculate_n_tokens(SF.col(\"article\"))\n",
    ")\n",
    "\n",
    "# Calculate instruction tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"instruction_token_count\", calculate_n_tokens(SF.col(\"instruction\"))\n",
    ")\n",
    "\n",
    "# Calculate generated tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"generated_token_count\", calculate_n_tokens(SF.col(\"cleaned_llm_summary\"))\n",
    ")\n",
    "\n",
    "# Cache DF\n",
    "articles_df.cache()\n",
    "articles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec175f1-54a4-4d95-b7e8-1cc8f0f65520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Display Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c0b40d-6f33-4c55-8689-607203b9924d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import datetime\n",
    "\n",
    "text_stats = (\n",
    "    articles_df.groupBy()\n",
    "    .agg(\n",
    "        SF.count(SF.col(\"id\")).alias(\"articles_count\"),\n",
    "        SF.sum(SF.col(\"article_token_count\")).alias(\"total_article_tokens\"),\n",
    "        SF.sum(SF.col(\"instruction_token_count\")).alias(\"total_instruction_tokens\"),\n",
    "        SF.sum(SF.col(\"generated_token_count\")).alias(\"total_generated_tokens\"),\n",
    "    )\n",
    "    .first()\n",
    ")\n",
    "\n",
    "human_elapsed_time = str(datetime.timedelta(seconds=inference_elapsed_time))\n",
    "\n",
    "print(\"-\" * 3 + \" Input \" + \"-\" * 3)\n",
    "print(f\"Total Article Count: {text_stats['articles_count']}\")\n",
    "print(f\"Articles Token Count: {text_stats['total_article_tokens']}\")\n",
    "print(f\"With Instructions Token Count: {text_stats['total_instruction_tokens']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 3 + \" Output \" + \"-\" * 3)\n",
    "print(f\"Generated Tokens Count: {text_stats['total_generated_tokens']}\")\n",
    "print(f\"Inference Elapsed Seconds: {inference_elapsed_time}\")\n",
    "print(f\"Inference Elapsed Time: {human_elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a855bc-704c-4432-80ec-f95b3a64450d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8c2ca3a-86fd-47dc-a7a7-94b20ffb070e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build DBFS path for the table\n",
    "save_location = f\"{STORAGE_PATH}/results\".split(\"/dbfs\")[-1]\n",
    "\n",
    "# Save Table\n",
    "articles_df.write.mode(\"overwrite\").save(save_location)\n",
    "\n",
    "# Register Table\n",
    "_ = spark.sql(f\"DROP TABLE IF EXISTS llm_batch_inference_results\")\n",
    "_ = spark.sql(\n",
    "    f\"CREATE TABLE llm_batch_inference_results USING DELTA LOCATION '{save_location}'\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1829729213475489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "transformer-batch-inference",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
