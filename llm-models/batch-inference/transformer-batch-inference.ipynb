{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd3363c-0e09-47f7-8953-9f2f887f624d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformer Based Batch Inference\n",
    "\n",
    "This notebook aims to show how you can run batch inference using Spark's distributed capabilites, with a multi-machine multi-gpu setup.\n",
    "\n",
    "Run on Databricks ML 14.0, with 4 A100 GPUs (1 Driver + 3 Worker) using _N24ads_A100_V4_ machines on Azure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632a9a3f-f2a3-4415-a9aa-b8780eafed00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Install & Upgrade Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96943394-f803-46e9-8cee-9336fe79a47d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers\n",
    "!pip install -q --upgrade accelerate\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3d7111-d60c-44ac-940e-e9127a4a3856",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GPU Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7fa476-5bf5-446d-a97d-502ddc2b26bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 16 17:41:57 2023       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A100 80G...  Off  | 00000001:00:00.0 Off |                    0 |\r\n| N/A   40C    P0    44W / 300W |      0MiB / 80994MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9033c85a-47bd-4cdf-a789-9f46ab5740a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Parameters\n",
    "\n",
    "The model name & tokenizer name below should be adjustable to most of the other models existing in the hugging face world.\n",
    "\n",
    "It would also make sense to change the prompt if there is a change to the model. This one is specifically designed for the LLAMA V2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74fd51cb-d946-4d34-8635-f5470d1cfd35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model Params\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "MAX_NEW_TOKENS = 300\n",
    "MIN_LENGTH = 0\n",
    "REPETITION_PENALTY = 1.2\n",
    "TEMPERATURE = 0.1\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "DO_SAMPLE = True\n",
    "USE_CACHE = True\n",
    "\n",
    "# Tokenizer Params\n",
    "TOKENIZER_NAME = MODEL_NAME\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "# Run Params (How many articles to use)\n",
    "MAX_EXAMPLES = 10000\n",
    "\n",
    "# Storage Params\n",
    "STORAGE_PATH = \"/dbfs/llm-examples\"\n",
    "\n",
    "# Instruction\n",
    "INSTRUCTION = \"\"\"Please provide a concise summary for the following article: {text}\"\"\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT_TEMPLATE = f\"\"\"<s>[INST]<<SYS>>\n",
    "You are a direct and honest assistant. Please provide concise and factual answers and just the answers.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "{INSTRUCTION}\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c61ab44-949e-4ee6-9dc4-2869a49c30d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Storage Operations\n",
    "\n",
    "Makes sure that the directory is cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6921db-f3bf-4c43-9571-1728a231122c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Remove existing files\n",
    "shutil.rmtree(\"/dbfs/llm-examples\")\n",
    "\n",
    "# Build the new directory\n",
    "os.makedirs(\"/dbfs/llm-examples\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4be6d00-bd7f-463f-b4dd-ed8672b29ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Huggingface Login\n",
    "\n",
    "This step can be skipped if your model doesn't require a login. LLAMA V2 does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a57d541-4da7-428a-ac83-b7f7d511434a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df48408c7e3e440ea006f8cf9c9dc5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login to hugging face\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Login the huggingface\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "973d0f78-5c75-4aa4-b892-9a4b8d36570a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Retrieve Articles Data\n",
    "\n",
    "CNN & Daily Mail articles data set from Hugging Face Datasets is retrieved and combined to make a pyspark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4ee291-7387-45f8-b5e3-c070870ff7da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:109: UserWarning: The dataset would be saved to both local disk and DBFS for better performance.\n  warnings.warn(\"The dataset would be saved to both local disk and DBFS for better performance.\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from datasets.utils import logging as dataset_logging, disable_progress_bar\n",
    "from pyspark.sql import functions as SF\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Disable verbose loggers\n",
    "dataset_logging.set_verbosity_error()\n",
    "disable_progress_bar()\n",
    "\n",
    "# Download dataset\n",
    "dataset = load_dataset(\n",
    "    path=\"cnn_dailymail\", name=\"3.0.0\", cache_dir=f\"{STORAGE_PATH}/hf\"\n",
    ")\n",
    "\n",
    "# Create spark dataframes\n",
    "train_df = spark.createDataFrame(data=dataset[\"train\"].to_pandas())\n",
    "val_df = spark.createDataFrame(data=dataset[\"validation\"].to_pandas())\n",
    "test_df = spark.createDataFrame(data=dataset[\"test\"].to_pandas())\n",
    "\n",
    "# Union for all data\n",
    "articles_df = train_df.union(val_df).union(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ffe435-7e6a-4127-a5eb-d7b6c29a50e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sample Data\n",
    "\n",
    "Deterministic sampling through ID hexing for consistent results & benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b64b75b-f05d-4785-9654-fe5b9cac703e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "import hashlib\n",
    "\n",
    "# Build function for creating a random column\n",
    "@SF.udf(\"string\")\n",
    "def generate_hex_from_string(input_string: str) -> str:\n",
    "    sha256 = hashlib.sha256()\n",
    "    sha256.update(input_string.encode(\"utf-8\"))\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "\n",
    "# Generate random string\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"random_string\", generate_hex_from_string(SF.col(\"id\"))\n",
    ")\n",
    "\n",
    "# Order by randomness and limit dataframe size\n",
    "articles_df = articles_df.orderBy(SF.col(\"random_string\")).limit(MAX_EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6d5212-466b-4d68-b2be-9bc29e1dfedd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Generate Prompts\n",
    "\n",
    "Prompts are applied with the article text to generate prepared instructions for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d8ca41-0af4-41e8-b954-44218c2da0e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Build function for generating instructions\n",
    "@SF.udf(\"string\")\n",
    "def generate_instructions(article):\n",
    "    return PROMPT_TEMPLATE.format(text=article)\n",
    "\n",
    "\n",
    "# Generate instructions\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"instruction\", generate_instructions(SF.col(\"article\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62ed7e9-7f9f-4659-a659-6d51a5c65e61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Execute Data Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07bd080-a45e-45dd-85f1-87145e4da39e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Trigger with action\n",
    "articles_df = spark.createDataFrame(articles_df.toPandas()).repartition(10)\n",
    "\n",
    "# Cache for performance\n",
    "articles_df.cache()\n",
    "print(f\"Number of Examples: {articles_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057d87e4-9e0a-4dab-8672-11f51fd5fb69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Download Model & Tokenizer\n",
    "\n",
    "Downloading the model and the tokizer helps when it comes to loading the model faster during the multi machine inference step.\n",
    "\n",
    "If the model and the tokenizer are in the same repository, only one download will occur. In some cases, for example for Falcon-7B, they can be different. In that case, the code downloads both to the location specified in params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e326895c-391c-4c1b-8fad-bf3fa9676cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from huggingface_hub.utils import (\n",
    "    disable_progress_bars as hfhub_disable_progress_bar,\n",
    "    logging as hf_logging,   \n",
    ")\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# Turn Off Info Logging for Transfomers\n",
    "hf_logging.set_verbosity_error()\n",
    "hfhub_disable_progress_bar()\n",
    "\n",
    "# Download the model \n",
    "local_model_path = f\"{STORAGE_PATH}/model/\"\n",
    "os.makedirs(local_model_path, exist_ok=True)\n",
    "model_download = snapshot_download(\n",
    "    repo_id=MODEL_NAME,\n",
    "    local_dir=local_model_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    ignore_patterns=\"*.safetensors*\", # This argument is specific to LLAMA. Other models might not need it.\n",
    "    max_workers=48\n",
    ")\n",
    "\n",
    "# Download the tokenizer\n",
    "if MODEL_NAME == TOKENIZER_NAME:\n",
    "    local_tokenizer_path = local_model_path\n",
    "else:\n",
    "    local_tokenizer_path = f\"{STORAGE_PATH}/tokenizer/\"\n",
    "    os.makedirs(local_tokenizer_path, exist_ok=True)\n",
    "    tokenizer_download = snapshot_download(\n",
    "        repo_id=TOKENIZER_NAME,\n",
    "        local_dir=local_tokenizer_path,\n",
    "        local_dir_use_symlinks=False,\n",
    "        max_workers=48\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea24216-9024-4ad1-b74f-629d72a38a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Model & Tokenizer\n",
    "\n",
    "Model and Tokenizer are loaded for the downloaded directory for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3c804b-c242-40e6-b2e9-3653f6b46152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911eedec13704a05af06cd2a03256f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3a3be069d14985a456a48987e34600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Params\n",
    "random_seed = 42\n",
    "\n",
    "# Random seed set\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    return_dict=True,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1b16cb-d267-4b35-9f13-3fed6a2e4f07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run For One\n",
    "\n",
    "Batch Generate functions takes a list of prompts, and returns a list of ouputs (generated_text). Generation parameters such a temperature and top_p are set within the function.\n",
    "\n",
    "Even though this example shows how to do a few examples, this function will be used during distributed inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d49fd56-5c18-4c55-95eb-a5b7768cee69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\nYou are a direct and honest assistant. Please provide concise and factual answers and just the answers.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n<</SYS>>\n\nPlease provide a concise summary for the following article: PUBLISHED: . 12:42 EST, 30 August 2012 . | . UPDATED: . 02:28 EST, 31 August 2012 . Guilty: Gordon Dyche has been sentenced to four years in prison for causing death by careless driving . A 'chancer' who killed four members of the same family when he shunted their car off the road and into a reservoir while rushing to work was jailed for four years yesterday. Mechanic Gordon Dyche, 24, had recently completed a driving ban when he tried to overtake two cars in a row on a winding country road because he was worried he would lose pay if he was late. He passed a Volkswagen Passat which was travelling at 40mph but his Mondeo clipped the back of a people carrier being driven by dedicated foster mother Denise Griffith as she attempted to turn right intoÂ  a lay-by to admire the view at a beauty spot. Her Peugeot 807 careered 20ft down a rocky bank and hit a boulder before somersaulting into the reservoir and sinking 50ft. Mrs Griffith's husband of 26 years Emyr, 66, a power station worker, her 84-year-old mother Phyllis Hooper and foster sons Peter Briscombe and Liam Govier, both 14, were trapped inside and drowned. Only Mrs Griffith, 55, and the family dog Milly survived by swimming to safety from Llyn Clywedog Reservoir near Llanidloes, mid Wales. Dyche, who had a history of minor driving offences, had completed a three month ban for driving without insurance just three weeks before the accident on the B4518 in April last year. He denied causing the deaths by careless driving but a jury found him guilty. He was cleared of four counts of causing death by dangerous driving. Judge Niclas Parry told Dyche at Caernarfon Crown Court he had put his own interests above the safety of others. 'You are a chancer behind the wheel,' the judge said. 'You are prepared to put your own interests above the lives of other road users. Bubbly: The biological mother of victim Liam Govier, 14, described him as a lovely, smiley little chap whose favourite school subject was cookery . 'You were late for work. You knew from previous experience the consequences would be docked wages.' Dyche, of Llanbrynmair, mid Wales, who . is married with a young child, was also handed another six month jail . term after it emerged he was serving a suspended prison sentence for . handling stolen goods at the time of the accident. He was also banned . from driving for four years. Tragedy: Autistic Peter Briscombe, aged 14,Â  died with his foster brother in the accident on April 20 last year . Gordon Dyche, left,Â  was found guilty of causing death by careless driving when he overtook Denise Griffith's, right, car which crashed into a reservoir killing her husband, mother and two foster sons . Support: Mrs Griffith arrives at Caernarfon Crown Court with her two brothers and other family members for the start of the trial earlier this week . 'Lonely place to be': Mrs Griffith returns to the scene of the crash, which happened when a driver hit her from behind while she was turning into a lay-by next to the reservoir . Drowned: Mrs Griffith's husband, . Emyr Griffith, foster sons, Peter Briscome and Liam Govier, and mother Phyllis Hooper (pictured), all died in the crash . After the verdicts a tearful Mrs . Griffith, who told the court her life had been 'ruined' by Dyche, said: . 'No words can express how much I miss my family and how much my life has . changed since that day in April 2011.' She and her husband, from Pontypridd, who were unable to have children of their own, fostered 98 children over 23 years. Liam's birth mother Alison Govier, 38, . said her son 'was a lovely bubbly boy'. She described Dyche as a menace . who 'should never have been on the road'. Peter's father Simon Briscombe, 43, of Ely, Cardiff, said: 'We are pleased he's been jailed but it's not long for four lives.' A . family friend of Denise Griffith, who did not want to be identified, . said after the verdict that Mrs Griffith could never be compensated for . her loss. â€˜Her life has been ruined. Of course she wanted the person responsible for what happened to be made accountable. â€˜But she isn't taking any great comfort in the outcome. At the end of the day, the poor lady has lost her family. â€˜ . In . a court hearing at Caernarfon Crown Court. On Tuesday Mrs Griffith . said: 'I know my life has been ruined. Itâ€™s the most lonely, lonely . place to be.' Grim search: Police officers at the scene where Mrs Griffith's people carrier plunged into Llyn Clywedog reservoir near Llanidloes on April 20 last year .\n[/INST]\nGordon Dyche, a 24-year-old mechanic from Mid-Wales, was sentenced to four years in prison for causing death by careless driving. Dyche was involved in a fatal car accident on April 20, 2011, when he shunted a car carrying five members of the same family off the road and into a reservoir. The victims included the family's patriarch, Emyr Griffith, his wife Denise, and their foster sons Peter Briscombe and Liam Govier. Dyche had recently completed a driving ban and was speeding to work when he attempted to overtake two cars in a row on a winding country road. His actions caused the car to collide with the Griffiths' vehicle, resulting in the tragic loss of life. During the trial, Dyche denied causing the deaths by careless driving, but a jury found him guilty. He was additionally given a six-month jail term for serving a suspended prison sentence for handling stolen goods at the time of the accident.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "\n",
    "# Get sample data\n",
    "sample_instructions = [x[0] for x in articles_df.select(\"instruction\").limit(2).collect()]\n",
    "\n",
    "# Define Inference Flow\n",
    "@torch.inference_mode()\n",
    "def batch_generate(batch_prompts, tokenizer=tokenizer, model=model):\n",
    "    batch = tokenizer.batch_encode_plus(\n",
    "        batch_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "        max_length=MAX_TOKENS\n",
    "    )\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "            temperature=TEMPERATURE,\n",
    "            min_length=MIN_LENGTH,\n",
    "            use_cache=USE_CACHE,\n",
    "            top_k=TOP_K,\n",
    "            repetition_penalty=REPETITION_PENALTY,\n",
    "        )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Check out one example\n",
    "print(batch_generate(sample_instructions[:1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646f6112-3d8c-4f12-916a-014627a58f82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Batch Test\n",
    "\n",
    "Optimal batch size changes depending on the GPU used. The GPU used in during this test has 80 GB of GPU memory, so going higher makes sense, however smaller machine like the A10s usually do better with smaller batch sizes.\n",
    "\n",
    "The code compares multiple batch sizes, and interation stops when out of memory error is raised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67371f1a-718b-4c72-9bbd-bed5202627ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 1, 'elapsed_time': 6.81, 'unit_time': 6.81, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 2, 'elapsed_time': 8.47, 'unit_time': 4.24, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 3, 'elapsed_time': 8.77, 'unit_time': 2.92, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 4, 'elapsed_time': 8.98, 'unit_time': 2.25, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 5, 'elapsed_time': 8.35, 'unit_time': 1.67, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 7, 'elapsed_time': 11.76, 'unit_time': 1.68, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 10, 'elapsed_time': 18.24, 'unit_time': 1.82, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 12, 'elapsed_time': 21.38, 'unit_time': 1.78, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 15, 'elapsed_time': 25.26, 'unit_time': 1.68, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 17, 'elapsed_time': 30.36, 'unit_time': 1.79, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 20, 'elapsed_time': 35.33, 'unit_time': 1.77, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 25, 'elapsed_time': 43.08, 'unit_time': 1.72, 'success': True}\n- - - - - - - - - - - - - - - - - - - - \n{'batch_size': 30, 'elapsed_time': 1.67, 'unit_time': 0.06, 'success': False}\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "\n",
    "# Get sample data\n",
    "sample_instructions = [x[0] for x in articles_df.select(\"instruction\").limit(50).collect()]\n",
    "\n",
    "def batch_size_optimiser():\n",
    "    batch_sizes = [1, 2, 3, 4, 5, 7, 10, 12, 15, 17, 20, 25, 30]\n",
    "    success = True\n",
    "    for size in batch_sizes:\n",
    "        start = time.perf_counter()\n",
    "        try:\n",
    "            batch_generate(batch_prompts=sample_instructions[:size])\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            success = False\n",
    "            break\n",
    "        finally:\n",
    "            elapsed = round(time.perf_counter() - start, 2)\n",
    "            unit_time = round(elapsed/size, 2)\n",
    "            yield {\"batch_size\": size, \"elapsed_time\": elapsed, \"unit_time\": unit_time, \"success\": success}\n",
    "\n",
    "for result in batch_size_optimiser():\n",
    "    print(\"- - \" * 10)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc57e4e-32bc-4a81-9a23-5bf57f9bba14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Select Batch Size\n",
    "\n",
    "Doesn't necessarily has to be the largest batch size that succeeded without running into an OOM error. It is probably better to choose the 2nd or 3rd largest successful batch size so that OOM errors can be reduced during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731ca081-ddb5-4d1d-8f12-0539c3c79871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the bactch size with minimum unit time\n",
    "OPTIMAL_BATCH_SIZE = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f498bba5-156e-4bd3-b01c-ce3e17ac54c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Distributed Inference Logic\n",
    "\n",
    "All of the generation logic is carried into a Pandas UDF. This helps with the set up on the workers. An iterator to interator architecture is followed to handle batching processes. \n",
    "\n",
    "In the case that the model runs into an OOM Error, the function handles the exception by returnin a OOM string as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be596558-8558-437a-a62d-e75507dbc7a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/functions.py:404: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# External Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "\n",
    "# Build Inference Function\n",
    "@SF.pandas_udf(\"string\", SF.PandasUDFType.SCALAR_ITER)\n",
    "def run_distributed_inference(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "\n",
    "    # External Imports\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    # Params\n",
    "    random_seed = 42\n",
    "\n",
    "    # Random seed set\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_path,\n",
    "        return_dict=True,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    for prompts in iterator:\n",
    "        prompts = prompts.to_list()\n",
    "        try:\n",
    "            output = batch_generate(\n",
    "                batch_prompts=prompts, \n",
    "                tokenizer=tokenizer, \n",
    "                model=model\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            # If out of memory, return a series of OOM strings that has the lenght of the input\n",
    "            output = [\"OOM\"] * len(prompts)\n",
    "\n",
    "        yield pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc8047e-87ab-41f4-851d-9df089d9d9a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inference Configurations\n",
    "\n",
    "Automatically undertands how many workers are available in the cluster, and adjusts partitions accordingly. This means that the setup portion of the Pandas UDF which loads the model and tokenizer gets run only once during inference, and the data processing is handled with the iterator.\n",
    "\n",
    "Max Records Per Batch configuration controls how big the batch sizes are going to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80654ef-411d-490a-b683-aa605cdb205a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Auto get number of workers\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Subtract 1 to exclude the driver\n",
    "num_workers = len(sc._jsc.sc().statusTracker().getExecutorInfos()) - 1  \n",
    "\n",
    "# Set the batch size for the Pandas UDF\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", OPTIMAL_BATCH_SIZE * 2)\n",
    "\n",
    "# Repartition\n",
    "articles_df = articles_df.repartition(num_workers)\n",
    "\n",
    "# Cache DF\n",
    "articles_df.cache()\n",
    "articles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a037004-60c7-4e64-aab0-9165ff0c5c2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run Distributed Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9c354e-24dd-48a8-be54-c96c453cf828",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Apply Inference UDF\n",
    "articles_df = (\n",
    "    articles_df\n",
    "    .withColumn(\"llm_summary\", run_distributed_inference(SF.col(\"instruction\")))\n",
    "\n",
    ")\n",
    "\n",
    "# Materilize and Execute\n",
    "inference_start_time = time.perf_counter()\n",
    "articles_pdf = articles_df.toPandas()\n",
    "inference_elapsed_time = round(time.perf_counter() - inference_start_time, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5622467-eca1-4ed7-83a0-de70a45fddf3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f147d8-c313-4409-ac9f-5f3c443401e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Go back to Spark\n",
    "articles_df = spark.createDataFrame(articles_pdf)\n",
    "\n",
    "# Cache DF\n",
    "articles_df.cache()\n",
    "articles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8c6d6f-dd6a-40be-8a84-6eced296c343",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb7e010-00d5-49f4-ae27-69db859527d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# UDF Build\n",
    "clean_llm_summary = SF.udf(lambda x: x.split(\"[/INST]\")[-1].strip(), \"string\")\n",
    "\n",
    "# Apply UDF\n",
    "articles_df = (\n",
    "    articles_df.withColumn(\n",
    "        \"cleaned_llm_summary\", clean_llm_summary(SF.col(\"llm_summary\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cb8325-721c-44ae-bca1-ff5db0704925",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calculate Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634f432c-e359-4729-8821-3c0141971cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@SF.udf(\"int\")\n",
    "def calculate_n_tokens(target_text):\n",
    "    return len(\n",
    "        tokenizer.encode_plus(\n",
    "            target_text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=True,\n",
    "            max_length=MAX_TOKENS,\n",
    "        )[\"input_ids\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Calculate article tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"article_token_count\", calculate_n_tokens(SF.col(\"article\"))\n",
    ")\n",
    "\n",
    "# Calculate instruction tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"instruction_token_count\", calculate_n_tokens(SF.col(\"instruction\"))\n",
    ")\n",
    "\n",
    "# Calculate generated tokens\n",
    "articles_df = articles_df.withColumn(\n",
    "    \"generated_token_count\", calculate_n_tokens(SF.col(\"cleaned_llm_summary\"))\n",
    ")\n",
    "\n",
    "# Cache DF\n",
    "articles_df.cache()\n",
    "articles_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec175f1-54a4-4d95-b7e8-1cc8f0f65520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Display Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4c0b40d-6f33-4c55-8689-607203b9924d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Input ---\nTotal Article Count: 10000\nArticles Token Count: 9972345\nWith Instructions Token Count: 10956621\n\n--- Output ---\nGenerated Tokens Count: 2979721\nInference Elapsed Seconds: 6484.8818\nInference Elapsed Time: 1:48:04.881800\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import datetime\n",
    "\n",
    "text_stats = (\n",
    "    articles_df.groupBy()\n",
    "    .agg(\n",
    "        SF.count(SF.col(\"id\")).alias(\"articles_count\"),\n",
    "        SF.sum(SF.col(\"article_token_count\")).alias(\"total_article_tokens\"),\n",
    "        SF.sum(SF.col(\"instruction_token_count\")).alias(\"total_instruction_tokens\"),\n",
    "        SF.sum(SF.col(\"generated_token_count\")).alias(\"total_generated_tokens\"),\n",
    "    )\n",
    "    .first()\n",
    ")\n",
    "\n",
    "human_elapsed_time = str(datetime.timedelta(seconds=inference_elapsed_time))\n",
    "\n",
    "print(\"-\" * 3 + \" Input \" + \"-\" * 3)\n",
    "print(f\"Total Article Count: {text_stats['articles_count']}\")\n",
    "print(f\"Articles Token Count: {text_stats['total_article_tokens']}\")\n",
    "print(f\"With Instructions Token Count: {text_stats['total_instruction_tokens']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 3 + \" Output \" + \"-\" * 3)\n",
    "print(f\"Generated Tokens Count: {text_stats['total_generated_tokens']}\")\n",
    "print(f\"Inference Elapsed Seconds: {inference_elapsed_time}\")\n",
    "print(f\"Inference Elapsed Time: {human_elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a855bc-704c-4432-80ec-f95b3a64450d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8c2ca3a-86fd-47dc-a7a7-94b20ffb070e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build DBFS path for the table\n",
    "save_location = f\"{STORAGE_PATH}/results\".split(\"/dbfs\")[-1]\n",
    "\n",
    "# Save Table\n",
    "articles_df.write.mode(\"overwrite\").save(save_location)\n",
    "\n",
    "# Register Table\n",
    "_ = spark.sql(f\"DROP TABLE IF EXISTS llm_batch_inference_results\")\n",
    "_ = spark.sql(\n",
    "    f\"CREATE TABLE llm_batch_inference_results USING DELTA LOCATION '{save_location}'\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1829729213475489,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "transformer-batch-inference",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
